{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1095834e",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff0e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import boto3\n",
    "import sys\n",
    "import time \n",
    "import duckdb\n",
    "import datetime\n",
    "import uuid\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pyathena import connect\n",
    "from sqlalchemy import create_engine, text\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from psycopg2 import sql\n",
    "from pyspark.sql import SparkSession\n",
    "from io import BytesIO\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc42ab",
   "metadata": {},
   "source": [
    "## Fun√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f6e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conex√£o ao banco de dados\n",
    "def test_connection(engine):\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Testar a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Listar as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"-\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba6811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para upload Parquet para S3\n",
    "def upload_parquet_to_s3(df: pd.DataFrame, layer_name: str, file_name: str = None):\n",
    "    bio = BytesIO()\n",
    "    df.to_parquet(bio, index=False, engine='pyarrow', compression='snappy')\n",
    "    bio.seek(0)\n",
    "\n",
    "    if not file_name:\n",
    "        ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        uid = uuid.uuid4().hex[:8]\n",
    "        file_name = f\"pnad_raw_{ts}_{uid}.parquet\"\n",
    "\n",
    "    # Construa a chave S3 corretamente\n",
    "    s3_key = f\"{layer_name}/{file_name}\"\n",
    "    s3_client.upload_fileobj(bio, s3_bucket, s3_key)\n",
    "    print(f\"‚úÖ RAW: s3://{s3_bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb934b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L√™ todos os arquivos Parquet de um prefixo S3 e concatena em um DataFrame.\n",
    "def read_parquet_from_s3(bucket, prefix):\n",
    "    all_dfs = []\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        if 'Contents' not in response:\n",
    "            print(f\"‚ùå Nenhum arquivo encontrado em s3://{bucket}/{prefix}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        for obj in response['Contents']:\n",
    "            # Pula se o objeto for uma pasta (termina com '/')\n",
    "            if obj['Key'].endswith('/'):\n",
    "                continue\n",
    "\n",
    "            print(f\"Lendo arquivo: s3://{bucket}/{obj['Key']}\")\n",
    "            obj_data = s3_client.get_object(Bucket=bucket, Key=obj['Key'])\n",
    "            df = pd.read_parquet(BytesIO(obj_data['Body'].read()))\n",
    "            all_dfs.append(df)\n",
    "\n",
    "        if all_dfs:\n",
    "            return pd.concat(all_dfs, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao ler arquivos do S3: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc23506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva um DataFrame em um arquivo Parquet e faz o upload para uma camada S3.\n",
    "def upload_to_s3_layer(df: pd.DataFrame, layer_name: str, file_name: str = None):\n",
    "    bio = BytesIO()\n",
    "    df.to_parquet(bio, index=False, engine='pyarrow', compression='snappy')\n",
    "    bio.seek(0)\n",
    "\n",
    "    if not file_name:\n",
    "        ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        uid = uuid.uuid4().hex[:8]\n",
    "        file_name = f\"pnad_{layer_name}_{ts}_{uid}.parquet\"\n",
    "\n",
    "    s3_key = f\"{layer_name}/{file_name}\"\n",
    "    s3_client.upload_fileobj(bio, s3_bucket, s3_key)\n",
    "    print(f\"‚úÖ Salvo na camada {layer_name}: s3://{s3_bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805c264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria banco de dados no glue\n",
    "def create_glue_database(database_name, description=None, location_uri=None):\n",
    "    try:\n",
    "        # Verifica se o banco j√° existe\n",
    "        existing_dbs = glue_client.get_databases()\n",
    "        db_names = [db['Name'] for db in existing_dbs['DatabaseList']]\n",
    "\n",
    "        if database_name in db_names:\n",
    "            print(f\"‚ö†Ô∏è O banco de dados '{database_name}' j√° existe no Glue.\")\n",
    "            return\n",
    "\n",
    "        # Cria o banco\n",
    "        params = {\n",
    "            'DatabaseInput': {\n",
    "                'Name': database_name,\n",
    "                'Description': description or f'Banco {database_name} criado via script Python.'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if location_uri:\n",
    "            params['DatabaseInput']['LocationUri'] = location_uri\n",
    "\n",
    "        glue_client.create_database(**params)\n",
    "        print(f\"‚úÖ Banco de dados '{database_name}' criado com sucesso!\\n\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Erro ao criar banco de dados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db87ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ßao para uploard de arquivos via glue crawler\n",
    "def create_or_update_crawler(crawler_name, role_arn, database_name, s3_target_path):\n",
    "    try:\n",
    "        # Verifica se o crawler j√° existe\n",
    "        existing_crawlers = glue_client.get_crawlers()\n",
    "        crawler_names = [c['Name'] for c in existing_crawlers['Crawlers']]\n",
    "\n",
    "        if crawler_name in crawler_names:\n",
    "            print(f\"‚ö†Ô∏è O crawler '{crawler_name}' j√° existe. Atualizando...\")\n",
    "            glue_client.update_crawler(\n",
    "                Name=crawler_name,\n",
    "                Role=role_arn,\n",
    "                DatabaseName=database_name,\n",
    "                Targets={'S3Targets': [{'Path': s3_target_path}]}\n",
    "            )\n",
    "            print(f\"‚úÖ Crawler '{crawler_name}' atualizado com sucesso!\\n\")\n",
    "        else:\n",
    "            print(f\"üöÄ Criando crawler '{crawler_name}'...\")\n",
    "            glue_client.create_crawler(\n",
    "                Name=crawler_name,\n",
    "                Role=role_arn,\n",
    "                DatabaseName=database_name,\n",
    "                Description=f\"Crawler para detectar tabelas na camada Gold em {s3_target_path}\",\n",
    "                Targets={'S3Targets': [{'Path': s3_target_path}]},\n",
    "                TablePrefix=\"\",\n",
    "                SchemaChangePolicy={\n",
    "                    'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                    'DeleteBehavior': 'LOG'\n",
    "                }\n",
    "            )\n",
    "            print(f\"‚úÖ Crawler '{crawler_name}' criado com sucesso!\\n\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Erro ao criar/atualizar crawler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "535a2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria job de execu√ß√£o do crawler\n",
    "def run_crawler_and_wait(crawler_name):\n",
    "    try:\n",
    "        print(f\"üöÄ Iniciando execu√ß√£o do crawler '{crawler_name}'...\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "        while True:\n",
    "            status = glue_client.get_crawler(Name=crawler_name)['Crawler']['State']\n",
    "            if status == 'READY':\n",
    "                print(f\"‚úÖ Crawler '{crawler_name}' finalizado com sucesso!\\n\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚è≥ Crawler em execu√ß√£o...\")\n",
    "                time.sleep(180)\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Erro ao executar o crawler: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3924f80",
   "metadata": {},
   "source": [
    "## Configura√ß√£o AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar as credencias do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configura√ß√£o storage_options\n",
    "storage_options = {\n",
    "    \"key\": os.getenv('aws_access_key_id'),\n",
    "    \"secret\": os.getenv('aws_secret_access_key'),\n",
    "    \"token\": os.getenv('aws_session_token')\n",
    "}\n",
    "\n",
    "# S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('aws_access_key_id'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_access_key'),\n",
    "    aws_session_token=os.getenv('aws_session_token'),\n",
    "    region_name=os.getenv('region')\n",
    ")\n",
    "\n",
    "#glue\n",
    "glue_client = boto3.client(\n",
    "    'glue',\n",
    "    aws_access_key_id=os.getenv('aws_access_key_id'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_access_key'),\n",
    "    aws_session_token=os.getenv('aws_session_token'),\n",
    "    region_name=os.getenv('region')\n",
    ")\n",
    "\n",
    "# PostgreSQL\n",
    "usuario_pg = os.getenv(\"POSTGRES_USER_PNAD\")\n",
    "senha_pg = os.getenv(\"POSTGRES_PASSWORD_PNAD\")\n",
    "host_pg = os.getenv(\"POSTGRES_HOST_PNAD\")\n",
    "porta_pg = os.getenv(\"POSTGRES_PORT_PNAD\")\n",
    "banco_pg = os.getenv(\"POSTGRES_DB_PNAD\")\n",
    "\n",
    "\n",
    "# Cria os clientes para os servi√ßos Glue e Athena\n",
    "iam_client = boto3.client('iam', region_name=os.getenv('region'))\n",
    "#glue_client = boto3.client('glue', region_name=os.getenv('region'))\n",
    "athena_client = boto3.client('athena', region_name=os.getenv('region'))\n",
    "role_arn = 'arn:aws:iam::992382762426:role/LabRole'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921cea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Configura√ß√µes S3 e camadas -------------------\n",
    "s3_bucket = 'fiaptechchallengefase3'\n",
    "\n",
    "# Camadas\n",
    "s3_raw = 'raw'\n",
    "s3_bronze = 'bronze'\n",
    "s3_silver = 'silver'\n",
    "s3_gold = 'gold'\n",
    "\n",
    "# Prefixos S3\n",
    "raw_prefix = f\"s3://{s3_bucket}/{s3_raw}/\"\n",
    "bronze_prefix = f\"s3://{s3_bucket}/{s3_bronze}/\"\n",
    "silver_prefix = f\"s3://{s3_bucket}/{s3_silver}/\"\n",
    "gold_prefix = f\"s3://{s3_bucket}/{s3_gold}/\"\n",
    "\n",
    "# Nome arquivos Parquet\n",
    "nome_arquivo_gold = 'pnad_final.parquet'\n",
    "caminho_saida_silver = silver_prefix + 'pnad.parquet'\n",
    "caminho_entrada_silver = silver_prefix + 'pnad.parquet'\n",
    "caminho_saida_gold = gold_prefix + nome_arquivo_gold\n",
    "caminho_completo_gold = gold_prefix + nome_arquivo_gold\n",
    "\n",
    "# ------------------- Configura√ß√µes RDS -------------------\n",
    "nome_tabela_inicial = \"pnad_covid\"\n",
    "nome_tabela_questionario = 'questionario_covid'\n",
    "nome_tabela_codigo_uf = 'codigo_uf'\n",
    "\n",
    "# ------------------- Configura√ß√µes de pipeline -------------------\n",
    "CHUNKSIZE = 100000  # N√∫mero de linhas por chunk para leitura do RDS\n",
    "\n",
    "\n",
    "# ------------------- Configura√ß√µes Glue e Athena -------------------\n",
    "database_name = 'db_fiap_challenge_glue'\n",
    "iam_role_name = 'FiapTechChallengeGlueRole'\n",
    "crawler_name = 'pnad_covid_crawler'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabbc9e",
   "metadata": {},
   "source": [
    "## Valida√ß√£o de conex√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7087bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado √† conta\n",
      "\n",
      "UserId: AROA6ODU7HW5AICBR3YUC:user4401657=geovanaferreira47@gmail.com\n",
      "Account: 992382762426\n",
      "Arn Completo: arn:aws:sts::992382762426:assumed-role/voclabs/user4401657=geovanaferreira47@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Valida√ß√£o conex√£o com a AWS atrav√©s do .env\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    sts_client = boto3.client(\n",
    "        'sts',\n",
    "        aws_access_key_id=os.getenv('aws_access_key_id'),\n",
    "        aws_secret_access_key=os.getenv('aws_secret_access_key'),\n",
    "        aws_session_token=os.getenv('aws_session_token'),\n",
    "        region_name=os.getenv('region')\n",
    "    )\n",
    "    \n",
    "    identity = sts_client.get_caller_identity()\n",
    "    print(\"‚úÖ Conectado √† conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn Completo:\", identity[\"Arn\"])\n",
    "\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"‚ùå Erro ao conectar √† AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "767491b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado com sucesso: PostgreSQL 17.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 12.4.0, 64-bit\n",
      "üìÑ Tabelas no banco:\n",
      "- questionario_covid\n",
      "- codigo_uf\n",
      "- pnad_covid\n"
     ]
    }
   ],
   "source": [
    "# Criar engine com banco \n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario_pg}:{senha_pg}@{host_pg}:{porta_pg}/{banco_pg}\")\n",
    "\n",
    "# Testar a conex√£o\n",
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d7d88",
   "metadata": {},
   "source": [
    "## Leitura de arquivos do git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c36f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando: PNAD_COVID_052020.zip\n",
      "‚úÖ Lido PNAD_COVID_052020.csv do ZIP PNAD_COVID_052020.zip\n",
      "Baixando: PNAD_COVID_062020.zip\n",
      "‚úÖ Lido PNAD_COVID_062020.csv do ZIP PNAD_COVID_062020.zip\n",
      "Baixando: PNAD_COVID_072020.zip\n",
      "‚úÖ Lido PNAD_COVID_072020.csv do ZIP PNAD_COVID_072020.zip\n",
      "Baixando: PNAD_COVID_082020.zip\n",
      "‚úÖ Lido PNAD_COVID_082020.csv do ZIP PNAD_COVID_082020.zip\n",
      "Baixando: PNAD_COVID_092020.zip\n",
      "‚úÖ Lido PNAD_COVID_092020.csv do ZIP PNAD_COVID_092020.zip\n",
      "Baixando: PNAD_COVID_102020.zip\n",
      "‚úÖ Lido PNAD_COVID_102020.csv do ZIP PNAD_COVID_102020.zip\n",
      "Baixando: PNAD_COVID_112020.zip\n",
      "‚úÖ Lido PNAD_COVID_112020.csv do ZIP PNAD_COVID_112020.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL da API que lista os arquivos da pasta\n",
    "api_url = \"https://api.github.com/repos/geoferreira1/fiap_tech_challenge_fase_3_novo/contents/covid/microdados\"\n",
    "\n",
    "# 1Ô∏è‚É£ Pegar o JSON da pasta\n",
    "response = requests.get(api_url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Erro ao acessar a API: {response.status_code}\")\n",
    "\n",
    "arquivos_json = response.json()\n",
    "\n",
    "# 2Ô∏è‚É£ Inicializar lista para armazenar todos os DataFrames\n",
    "dfs = []\n",
    "colunas_comuns = None  # para armazenar as colunas que existem em todos os CSVs\n",
    "\n",
    "# 3Ô∏è‚É£ Iterar sobre cada arquivo listado\n",
    "for arquivo in arquivos_json:\n",
    "    if arquivo['name'].endswith('.zip'):\n",
    "        zip_url = arquivo['download_url']\n",
    "        print(f\"Baixando: {arquivo['name']}\")\n",
    "\n",
    "        # Baixar o arquivo ZIP em mem√≥ria\n",
    "        r = requests.get(zip_url)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"‚ùå Erro ao baixar {arquivo['name']}\")\n",
    "            continue\n",
    "        \n",
    "        zip_bytes = BytesIO(r.content)\n",
    "        \n",
    "        # Abrir ZIP e ler todos os CSVs dentro\n",
    "        with ZipFile(zip_bytes) as zip_file:\n",
    "            for csv_name in zip_file.namelist():\n",
    "                if csv_name.endswith('.csv'):\n",
    "                    with zip_file.open(csv_name) as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                        \n",
    "                        # Atualizar colunas comuns\n",
    "                        #if colunas_comuns is None:\n",
    "                        #    colunas_comuns = set(df.columns)\n",
    "                        #else:\n",
    "                        #    colunas_comuns &= set(df.columns)  # interse√ß√£o das colunas\n",
    "                        \n",
    "                        dfs.append(df)\n",
    "                        print(f\"‚úÖ Lido {csv_name} do ZIP {arquivo['name']}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Filtrar apenas as colunas comuns antes de concatenar\n",
    "#dfs_filtrados = [df[list(colunas_comuns)] for df in dfs]\n",
    "\n",
    "# 5Ô∏è‚É£ Concatenar todos os DataFrames em um √∫nico\n",
    "df_completo = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 6Ô∏è‚É£ Mostrar as primeiras linhas\n",
    "df_completo.head()\n",
    "\n",
    "# 7Ô∏è‚É£ Filtra os √∫ltimos 3 meses\n",
    "df_final1 = df_completo[df_completo['V1013'].isin([9, 8, 7])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac650ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lendo tabela com os c√≥digos do IBGE de: https://raw.githubusercontent.com/geoferreira1/fiap_tech_challenge_fase_3/refs/heads/main/covid/codigo_uf.csv\n",
      "Data Frame criado com sucesso\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminho do Github com dados do c√≥digo IBGE UF\n",
    "link_codigo_uf = 'https://raw.githubusercontent.com/geoferreira1/fiap_tech_challenge_fase_3/refs/heads/main/covid/codigo_uf.csv'\n",
    "\n",
    "# Ler arquivos CSV Codigo IBGE e gerar Data Frame\n",
    "print(f\"\\nLendo tabela com os c√≥digos do IBGE de: {link_codigo_uf}\")\n",
    "df_uf = pd.read_csv(link_codigo_uf, sep=\",\")\n",
    "print(\"Data Frame criado com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266ee19",
   "metadata": {},
   "source": [
    "## Cria pastas no s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b96b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Processo de Valida√ß√£o e Cria√ß√£o no S3\n",
      "\n",
      "Validando o bucket 'fiaptechchallengefase3'\n",
      "‚û°Ô∏è  Bucket 'fiaptechchallengefase3' j√° existe\n",
      "\n",
      "Validando as subpastas no bucket 'fiaptechchallengefase3'\n",
      "‚û°Ô∏è Pasta 'raw/' j√° existe\n",
      "‚û°Ô∏è Pasta 'bronze/' j√° existe\n",
      "‚û°Ô∏è Pasta 'silver/' j√° existe\n",
      "‚û°Ô∏è Pasta 'gold/' j√° existe\n",
      "\n",
      "Processo Finalizado\n"
     ]
    }
   ],
   "source": [
    "# Criar Bucket e subpastas\n",
    "\n",
    "# Lista com camadas no s3\n",
    "camadas = [s3_raw, s3_bronze, s3_silver, s3_gold]\n",
    "\n",
    "# Armazenar a regi√£o da conex√£o com a AWS\n",
    "aws_region = s3_client.meta.region_name\n",
    "\n",
    "print(f\"\\nIniciando Processo de Valida√ß√£o e Cria√ß√£o no S3\")\n",
    "\n",
    "print(f\"\\nValidando o bucket '{s3_bucket}'\")\n",
    "bucket_pronto = False\n",
    "\n",
    "# Validar se o Bucket esta criado\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"‚û°Ô∏è  Bucket '{s3_bucket}' j√° existe\")\n",
    "    bucket_pronto = True\n",
    "\n",
    "except ClientError as e:\n",
    "    \n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(f\"Bucket '{s3_bucket}' n√£o encontrado. Tentando criar\")\n",
    "\n",
    "        try:\n",
    "            if aws_region == \"us-east-1\":\n",
    "                s3_client.create_bucket(Bucket=s3_bucket)\n",
    "\n",
    "            else:\n",
    "                location = {'LocationConstraint': aws_region}\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=s3_bucket,\n",
    "                    CreateBucketConfiguration=location\n",
    "                )\n",
    "            print(f\"‚úÖ Bucket '{s3_bucket}' criado com sucesso!\")\n",
    "            bucket_pronto = True\n",
    "\n",
    "        except Exception as create_e:\n",
    "            print(f\"‚ùå Falha ao tentar criar o bucket: {create_e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Erro de permiss√£o ou outro problema ao verificar o bucket: {e}\")\n",
    "\n",
    "# Validar se as subpastas est√£o criadas\n",
    "if bucket_pronto:\n",
    "    print(f\"\\nValidando as subpastas no bucket '{s3_bucket}'\")\n",
    "\n",
    "    for nome_pasta in camadas:\n",
    "        chave_pasta = nome_pasta if nome_pasta.endswith('/') else nome_pasta + '/'\n",
    "        \n",
    "        try:\n",
    "            s3_client.head_object(Bucket=s3_bucket, Key=chave_pasta)\n",
    "            print(f\"‚û°Ô∏è Pasta '{chave_pasta}' j√° existe\")\n",
    "\n",
    "        except ClientError as e:\n",
    "\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                try:\n",
    "                    s3_client.put_object(\n",
    "                        Bucket=s3_bucket,\n",
    "                        Key=chave_pasta,\n",
    "                        Body=''\n",
    "                    )\n",
    "                    print(f\"‚úÖ Pasta '{chave_pasta}' criada com sucesso\")\n",
    "\n",
    "                except Exception as create_e:\n",
    "                    print(f\"‚ùå Falha ao TENTAR CRIAR a pasta '{chave_pasta}': {create_e}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ùå Erro ao verificar a pasta '{chave_pasta}': {e}\")\n",
    "else:\n",
    "    print(\"\\nCria√ß√£o das pastas abortada, pois houve um problema com o bucket\")\n",
    "\n",
    "print(\"\\nProcesso Finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf447a",
   "metadata": {},
   "source": [
    "## Insere dados no RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a0be2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataFrame inserido com sucesso na tabela 'codigo_uf' do RDS!\n"
     ]
    }
   ],
   "source": [
    "# Inserir no RDS (se a tabela j√° existir, substitui)\n",
    "df_uf.to_sql(\n",
    "    nome_tabela_codigo_uf,\n",
    "    con=engine,\n",
    "    if_exists='replace',  # 'replace' = substitui a tabela se j√° existir\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataFrame inserido com sucesso na tabela '{nome_tabela_codigo_uf}' do RDS!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20ea0b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataFrame inserido com sucesso na tabela 'pnad_covid' do RDS!\n"
     ]
    }
   ],
   "source": [
    "# Inserir no RDS (se a tabela j√° existir, substitui)\n",
    "df_final.to_sql(\n",
    "    nome_tabela_inicial,\n",
    "    con=engine,\n",
    "    if_exists='replace',  # 'replace' = substitui a tabela se j√° existir\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataFrame inserido com sucesso na tabela '{nome_tabela_inicial}' do RDS!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4a3d5",
   "metadata": {},
   "source": [
    "## Importa arquivos do RDS para s3 (Bronze -> Silver -> Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485e9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Upload chunk #1 com 100000 linhas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/8wnxmp196n904zdcm654qfbc0000gn/T/ipykernel_7212/1861067952.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012038Z_39a1b874.parquet\n",
      "üì¶ Upload chunk #2 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012042Z_85d1257f.parquet\n",
      "üì¶ Upload chunk #3 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012045Z_1ab76f0c.parquet\n",
      "üì¶ Upload chunk #4 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012048Z_9c824e44.parquet\n",
      "üì¶ Upload chunk #5 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012052Z_238e9b1c.parquet\n",
      "üì¶ Upload chunk #6 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012055Z_a330b021.parquet\n",
      "üì¶ Upload chunk #7 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012057Z_c23f875d.parquet\n",
      "üì¶ Upload chunk #8 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012100Z_a0099620.parquet\n",
      "üì¶ Upload chunk #9 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012103Z_e59aebab.parquet\n",
      "üì¶ Upload chunk #10 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012106Z_5091ae07.parquet\n",
      "üì¶ Upload chunk #11 com 100000 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012109Z_9194e1c3.parquet\n",
      "üì¶ Upload chunk #12 com 49197 linhas\n",
      "‚úÖ RAW: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012111Z_6d153dc9.parquet\n"
     ]
    }
   ],
   "source": [
    "sql = f\"SELECT * FROM {nome_tabela_inicial}\"\n",
    "chunk_iter = pd.read_sql_query(text(sql), engine, chunksize=CHUNKSIZE)\n",
    "for idx, chunk in enumerate(chunk_iter, start=1):\n",
    "    print(f\"üì¶ Upload chunk #{idx} com {len(chunk)} linhas\")\n",
    "    upload_parquet_to_s3(chunk, s3_raw) # Passe a vari√°vel `s3_raw` que cont√©m a string \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a transforma√ß√£o da camada Raw para a camada Bronze...\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012038Z_39a1b874.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012042Z_85d1257f.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012045Z_1ab76f0c.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012048Z_9c824e44.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012052Z_238e9b1c.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012055Z_a330b021.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012057Z_c23f875d.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012100Z_a0099620.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012103Z_e59aebab.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012106Z_5091ae07.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012109Z_9194e1c3.parquet\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/raw/pnad_raw_20251004T012111Z_6d153dc9.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/8wnxmp196n904zdcm654qfbc0000gn/T/ipykernel_7212/4150293736.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados brutos lidos. Iniciando a limpeza para a camada Bronze...\n",
      "‚úÖ Removidas 0 linhas duplicadas.\n",
      "‚úÖ Valores nulos tratados.\n",
      "‚úÖ Tipos de dados padronizados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/8wnxmp196n904zdcm654qfbc0000gn/T/ipykernel_7212/308496153.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Salvo na camada bronze: s3://fiaptechchallengefase3/bronze/pnad_bronze_20251004T012157Z_6e1de45b.parquet\n",
      "‚úÖ Processo de transforma√ß√£o de Raw para Bronze conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# --- Execu√ß√£o do Script ---\n",
    "print(\"Iniciando a transforma√ß√£o da camada Raw para a camada Bronze...\")\n",
    "\n",
    "# 1. L√™ os dados da camada Raw\n",
    "df_raw = read_parquet_from_s3(s3_bucket, s3_raw)\n",
    "\n",
    "if not df_raw.empty:\n",
    "    print(\"Dados brutos lidos. Iniciando a limpeza para a camada Bronze...\")\n",
    "\n",
    "    # 2. Realiza a transforma√ß√£o e limpeza (Bronze)\n",
    "    # Exemplo de limpeza: Remover linhas duplicadas\n",
    "    df_bronze = df_raw.drop_duplicates()\n",
    "    print(f\"‚úÖ Removidas {len(df_raw) - len(df_bronze)} linhas duplicadas.\")\n",
    "\n",
    "    # Exemplo de tratamento de valores nulos (substituindo NaN por -1)\n",
    "    df_bronze = df_bronze.fillna(-1)\n",
    "    print(\"‚úÖ Valores nulos tratados.\")\n",
    "\n",
    "    # Exemplo de convers√£o de tipos (garantindo que V1008 seja int)\n",
    "    df_bronze['V1008'] = pd.to_numeric(df_bronze['V1008'], errors='coerce').astype('Int64')\n",
    "    print(\"‚úÖ Tipos de dados padronizados.\")\n",
    "\n",
    "\n",
    "    # 3. Salva os dados processados na camada Bronze\n",
    "    upload_to_s3_layer(df_bronze, s3_bronze)\n",
    "    print(\"‚úÖ Processo de transforma√ß√£o de Raw para Bronze conclu√≠do.\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado para processar. Verifique a camada Raw.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9930195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a transforma√ß√£o da camada Bronze para a camada Silver...\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/bronze/pnad_bronze_20251004T012157Z_6e1de45b.parquet\n",
      "Dados brutos lidos. Iniciando os filtros para a camada Silver...\n",
      "\n",
      "Iniciando enriquecimento dos dados com merge com o Data Frame dicionadio de uf\n",
      "‚úÖ Enriquecimento feito com sucesso!\n",
      "‚úÖ Normaliza√ß√£o das colunas feita com sucesso!\n",
      "‚úÖ Sele√ß√£o de colunas feito com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/8wnxmp196n904zdcm654qfbc0000gn/T/ipykernel_5343/2023787219.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Salvo na camada silver: s3://fiaptechchallengefase3/silver/pnad_silver_20251005T170937Z_88538373.parquet\n",
      "‚úÖ Processo de transforma√ß√£o de Bronze para Silver conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# --- Execu√ß√£o do Script ---\n",
    "print(\"Iniciando a transforma√ß√£o da camada Bronze para a camada Silver...\")\n",
    "\n",
    "# 1. L√™ os dados da camada Bronze\n",
    "df_bronze = read_parquet_from_s3(s3_bucket, s3_bronze)\n",
    "\n",
    "if not df_bronze.empty:\n",
    "    print(\"Dados brutos lidos. Iniciando os filtros para a camada Silver...\")\n",
    "\n",
    "    # 2. Realiza a filtros (Silver)\n",
    "    # Relacionar os Data Frame\n",
    "    print(\"\\nIniciando enriquecimento dos dados com merge com o Data Frame dicionadio de uf\")\n",
    "    df_silver = pd.merge(\n",
    "        df_bronze,\n",
    "        df_uf,\n",
    "        how='left',\n",
    "        left_on='UF', \\\n",
    "        right_on='C√≥digo'\n",
    "    )\n",
    "\n",
    "    # Remover colunas n√£o necessarias e ajustar o nome das colunas\n",
    "    df_silver = df_silver.drop(columns=[\"C√≥digo\"])\n",
    "    df_silver = df_silver.rename(columns={\"UF_x\": \"UF\", \"UF_y\": \"Estado\", \"Regi√£o\": \"Regiao\"})\n",
    "    print(f\"‚úÖ Enriquecimento feito com sucesso!\")\n",
    "\n",
    "    # Copiar e padronizar colunas para min√∫sculas\n",
    "    df_silver_normalizado = df_silver.copy()\n",
    "    df_silver_normalizado.columns = df_silver_normalizado.columns.str.lower()\n",
    "    print(f\"‚úÖ Normaliza√ß√£o das colunas feita com sucesso!\")\n",
    "\n",
    "    # Manter colunas:\n",
    "    colunas_manter = [\n",
    "        'ano','v1013','v1012','uf','capital','rm_ride','estado','sigla','regiao','a002','a003','a004','a005',\n",
    "        'a006','b002','b005','b006','b007','b008','b009b','b009d','b009f','b0011','b0012','b0013','b0014','b0015',\n",
    "        'b0016','b0017','b0018','b0019','b00110','b00111','b00112','b00113','b0101','b0102','b0103','b0104','b0105','b0106','c007b','f001','b011','d0051', 'c01011',\n",
    "    ]\n",
    "\n",
    "    df_silver_normalizado = df_silver_normalizado[colunas_manter]\n",
    "    print(f\"‚úÖ Sele√ß√£o de colunas feito com sucesso!\")\n",
    "\n",
    "    # 3. Salva os dados processados na camada silver\n",
    "    upload_to_s3_layer(df_silver_normalizado, s3_silver)\n",
    "    print(\"‚úÖ Processo de transforma√ß√£o de Bronze para Silver conclu√≠do.\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado para processar. Verifique a camada Silver.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c1e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a transforma√ß√£o da camada Silver para a camada Gold...\n",
      "Lendo arquivo: s3://fiaptechchallengefase3/silver/pnad_silver_20251005T170937Z_88538373.parquet\n",
      "Iniciando o ETL para a camada Gold...\n",
      "‚úÖ Processo de mapeamento de dicion√°rios conclu√≠do.\n",
      "‚úÖ Processo de renomear as colunas conclu√≠do.\n",
      "‚úÖ Processo de classificar as colunas conclu√≠do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/8wnxmp196n904zdcm654qfbc0000gn/T/ipykernel_5343/2023787219.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Salvo na camada gold: s3://fiaptechchallengefase3/gold/pnad_gold_20251005T171304Z_e289285d.parquet\n",
      "‚úÖ Processo de transforma√ß√£o de Silver para Gold conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# --- Execu√ß√£o do Script ---\n",
    "print(\"Iniciando a transforma√ß√£o da camada Silver para a camada Gold...\")\n",
    "\n",
    "# 1. L√™ os dados da camada Silver\n",
    "df_silver = read_parquet_from_s3(s3_bucket, s3_silver)\n",
    "\n",
    "if not df_silver.empty:\n",
    "    print(\"Iniciando o ETL para a camada Gold...\")\n",
    "\n",
    "    # 2. Faz as tradu√ß√µes de c√≥digos para texto e renomeia as colunas\n",
    "    \n",
    "    # Dicion√°rios de mapeamento\n",
    "    mapeamento_sexo = {1: 'Masculino', 2: 'Feminino'}\n",
    "    mapeamento_raca_cor = {1: 'Branca', 2: 'Preta', 3: 'Amarela', 4: 'Parda', 5: 'Ind√≠gena'}\n",
    "    mapeamento_escolaridade = {1: 'Sem instru√ß√£o', 2: 'Ensino Fundamental incompleto', 3: 'Ensino Fundamental completo', 4: 'Ensino M√©dio incompleto', 5: 'Ensino M√©dio completo', 6: 'Ensino Superior incompleto', 7: 'Ensino Superior completo', 8: 'P√≥s gradua√ß√£o, mestrado ou doutorado'}\n",
    "    mapeamento_tipo_instituicao = {1: 'P√∫blica', 2: 'Privada'}\n",
    "    mapeamento_sim_nao_ignorado = {1: 'Sim', 2: 'N√£o'}\n",
    "    mapeamento_teste_resultado = {1: 'Positivo', 2: 'Negativo', 3: 'Inconclusivo', 4: 'Aguardando resultado'}\n",
    "    mapeamento_trabalha_atualmente = {1: 'Sim, carteira assinada', 2: 'Sim, servidor p√∫blico', 3: 'N√£o'}\n",
    "    mapeamento_moradia = {1: 'Pr√≥pria', 2: 'Pr√≥pria', 3: 'Aluguel', 4: 'Cedido', 5: 'Cedido', 6: 'Cedido'}\n",
    "    mapeamento_isolamento = {1: 'N√£o fez restri√ß√£o, levou vida normal como antes da pandemia', 2: 'Reduziu o contato com as pessoas, mas continuou saindo de casa para trabalho ou atividades n√£o essenciais e/ou recebendo visitas', 3: 'Ficou em casa e s√≥ saiu em caso de necessidade b√°sica', 4: 'Ficou rigorosamente em casa'}\n",
    "    mapeamento_auxilio_social = {1: 'Sim', 2: 'N√£o'}\n",
    "    mapeamento_faixa_salarial = {0:'0-100', 1: '101 - 300', 2: '301 - 600', 3: '601 - 800', 4: '801 - 1.600', 5: '1.601 - 3.000', 6: '3.001 - 10.000', 7:'Acima de 10.000',8:'Acima de 10.000',9:'Acima de 10.000'}\n",
    "    print(\"‚úÖ Processo de mapeamento de dicion√°rios conclu√≠do.\")\n",
    "\n",
    "    # Dicion√°rio de renomea√ß√£o de colunas\n",
    "    col_mapping = {\n",
    "        'v1013': 'mes_pesquisa', 'v1012': 'semana_mes',\n",
    "        'a002': 'idade', 'a003': 'sexo', 'a004': 'cor', 'a005': 'escolaridade', \n",
    "        'b002': 'buscou_auxilio_medico', 'b005': 'precisou_de_internacao', 'b006': 'precisou_de_sedacao',\n",
    "        'b007': 'plano_de_saude', 'b008': 'realizou_teste_covid', 'b009b': 'resultado_teste_swab',\n",
    "        'b009d': 'resultado_teste_dedo', 'b009f': 'resultado_teste_veia', 'b0011': 'frebre_semana_anterior',\n",
    "        'b0012': 'tosse_semana_anterior', 'b0013': 'dor_de_garganta_semana_anterior',\n",
    "        'b0014': 'dificuldade_de_respirar_semana_anterior', 'b0015': 'dor_de_cabeca_semana_anterior',\n",
    "        'b0016': 'dor_no_peito_semana_anterior', 'b0017': 'nausea_semana_anterior',\n",
    "        'b0018': 'nariz_constipado_semana_anterior', 'b0019': 'fadiga_semana_anterior',\n",
    "        'b00110': 'dor_nos_olhos_semana_anterior', 'b00111': 'perda_olfato_paladar_semana_anterior',\n",
    "        'b00112': 'dor_muscular_semana_anterior', 'b00113': 'diarreia_semana_anterior',\n",
    "        'b0101': 'diabetes', 'b0102': 'hipertensao', 'b0103': 'doenca_respiratoria',\n",
    "        'b0104': 'doencas_cardiacas', 'b0105': 'depressao', 'b0106': 'cancer',\n",
    "        'c007b': 'trabalha_atualmente', 'f001': 'modaria','b011':'isolamento_social','d0051': 'auxilio_emergencial','c01011':'faixa_salarial'\n",
    "    }\n",
    "    \n",
    "    df_gold = df_silver.copy()\n",
    "    \n",
    "    # Renomear as colunas\n",
    "    df_gold.rename(columns=col_mapping, inplace=True)\n",
    "    print(\"‚úÖ Processo de renomear as colunas conclu√≠do.\")\n",
    "\n",
    "    # Aplicar as tradu√ß√µes\n",
    "    df_gold['sexo'] = df_gold['sexo'].map(mapeamento_sexo).fillna('Desconhecido')\n",
    "    df_gold['cor'] = df_gold['cor'].map(mapeamento_raca_cor).fillna('Desconhecido')\n",
    "    df_gold['escolaridade'] = df_gold['escolaridade'].map(mapeamento_escolaridade).fillna('Desconhecido')\n",
    "    df_gold['buscou_auxilio_medico'] = df_gold['buscou_auxilio_medico'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['precisou_de_internacao'] = df_gold['precisou_de_internacao'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['precisou_de_sedacao'] = df_gold['precisou_de_sedacao'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['plano_de_saude'] = df_gold['plano_de_saude'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['realizou_teste_covid'] = df_gold['realizou_teste_covid'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['resultado_teste_swab'] = df_gold['resultado_teste_swab'].map(mapeamento_teste_resultado).fillna('Desconhecido')\n",
    "    df_gold['resultado_teste_dedo'] = df_gold['resultado_teste_dedo'].map(mapeamento_teste_resultado).fillna('Desconhecido')\n",
    "    df_gold['resultado_teste_veia'] = df_gold['resultado_teste_veia'].map(mapeamento_teste_resultado).fillna('Desconhecido')\n",
    "    df_gold['frebre_semana_anterior'] = df_gold['frebre_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['tosse_semana_anterior'] = df_gold['tosse_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dor_de_garganta_semana_anterior'] = df_gold['dor_de_garganta_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dificuldade_de_respirar_semana_anterior'] = df_gold['dificuldade_de_respirar_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dor_de_cabeca_semana_anterior'] = df_gold['dor_de_cabeca_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dor_no_peito_semana_anterior'] = df_gold['dor_no_peito_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['nausea_semana_anterior'] = df_gold['nausea_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['nariz_constipado_semana_anterior'] = df_gold['nariz_constipado_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['fadiga_semana_anterior'] = df_gold['fadiga_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dor_nos_olhos_semana_anterior'] = df_gold['dor_nos_olhos_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['perda_olfato_paladar_semana_anterior'] = df_gold['perda_olfato_paladar_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['dor_muscular_semana_anterior'] = df_gold['dor_muscular_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['diarreia_semana_anterior'] = df_gold['diarreia_semana_anterior'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['diabetes'] = df_gold['diabetes'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['hipertensao'] = df_gold['hipertensao'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['doenca_respiratoria'] = df_gold['doenca_respiratoria'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['doencas_cardiacas'] = df_gold['doencas_cardiacas'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['depressao'] = df_gold['depressao'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['cancer'] = df_gold['cancer'].map(mapeamento_sim_nao_ignorado).fillna('Desconhecido')\n",
    "    df_gold['trabalha_atualmente'] = df_gold['trabalha_atualmente'].map(mapeamento_trabalha_atualmente).fillna('Desconhecido')\n",
    "    df_gold['isolamento_social'] = df_gold['isolamento_social'].map(mapeamento_isolamento).fillna('Desconhecido')\n",
    "    df_gold['auxilio_emergencial'] = df_gold['auxilio_emergencial'].map(mapeamento_auxilio_social).fillna('Desconhecido')\n",
    "    df_gold['faixa_salarial'] = df_gold['faixa_salarial'].map(mapeamento_faixa_salarial).fillna('Desconhecido')\n",
    "    print(\"‚úÖ Processo de classificar as colunas conclu√≠do.\")\n",
    "\n",
    "    # 3. Salva os dados processados na camada silver\n",
    "    upload_to_s3_layer(df_gold, s3_gold)\n",
    "    print(\"‚úÖ Processo de transforma√ß√£o de Silver para Gold conclu√≠do.\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado para processar. Verifique a camada Silver.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aed3d7",
   "metadata": {},
   "source": [
    "## Importa tabela da Gold para banco (An√°lise dos dados por RDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc182f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo: s3://fiaptechchallengefase3/gold/pnad_gold_20251005T171304Z_e289285d.parquet\n",
      "‚úÖ DataFrame inserido com sucesso na tabela 'questionario_covid' do RDS!\n"
     ]
    }
   ],
   "source": [
    "# Inserir no RDS (se a tabela j√° existir, substitui)\n",
    "\n",
    "# 1. L√™ os dados da camada Silver\n",
    "df_gold = read_parquet_from_s3(s3_bucket, s3_gold)\n",
    "\n",
    "df_gold.to_sql(\n",
    "    nome_tabela_questionario,\n",
    "    con=engine,\n",
    "    if_exists='replace',  # 'replace' = substitui a tabela se j√° existir\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataFrame inserido com sucesso na tabela '{nome_tabela_questionario}' do RDS!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdb1c6",
   "metadata": {},
   "source": [
    "## Importa tabela da Gold para Glue (An√°lise dos dados por Athena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada656fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Banco de dados 'db_fiap_challenge_glue' criado com sucesso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_name = database_name\n",
    "db_description = \"Este √© um banco de dados para o Athena consumir os dados da gold do s3.\"\n",
    "create_glue_database(db_name, db_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0754c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Criando crawler 'pnad_covid_crawler'...\n",
      "‚úÖ Crawler 'pnad_covid_crawler' criado com sucesso!\n",
      "\n",
      "üöÄ Iniciando execu√ß√£o do crawler 'pnad_covid_crawler'...\n",
      "‚è≥ Crawler em execu√ß√£o...\n",
      "‚úÖ Crawler 'pnad_covid_crawler' finalizado com sucesso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cria ou atualiza o crawler\n",
    "create_or_update_crawler(\n",
    "    crawler_name=crawler_name,\n",
    "    role_arn=role_arn,\n",
    "    database_name=database_name,\n",
    "    s3_target_path=gold_prefix\n",
    ")\n",
    "# Executa o crawler e aguarda finaliza√ß√£o\n",
    "run_crawler_and_wait(crawler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e2405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech_fase_3_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
